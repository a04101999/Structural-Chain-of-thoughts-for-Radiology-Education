{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install scikit-learn\n",
    "% pip install matplotlib\n",
    "% pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupiter notebook evaluates the performance of the baseline zero-shot and few-shot prompting on the synthesized error dataset, to compare the effectiveness of the SCot framework in improving multimodal reasoning across different LLM/LMM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the path's with the correct paths to your results generated using each prompting method with the respective models. Wherever the path is mentioned `with open`, please replace with your path to the respective file.\n",
    "\n",
    "```python\n",
    "with open(\".\\llama3.211BV\\llama3.2_11B_zshot_predictions.json\") as file:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label_dict(label_dict):\n",
    "    # Define the mapping between class labels and the corresponding descriptions\n",
    "    label_mapping = {\n",
    "        'class_label_1': 'Missed abnormality due to missing fixation',\n",
    "        'class_label_2': 'Missed abnormality due to reduced fixation',\n",
    "        'class_label_3': 'Missed abnormality due to incomplete knowledge'\n",
    "    }\n",
    "\n",
    "    transformed_dict = {}\n",
    "\n",
    "    for key, value in label_dict.items():\n",
    "        transformed_dict[key] = {}\n",
    "\n",
    "        # Map each label to its new description based on `label_mapping`\n",
    "        for class_label, new_description in label_mapping.items():\n",
    "            transformed_dict[key][new_description] = value.get(class_label, 0)\n",
    "        \n",
    "        # Set \"No missing abnormality\" based on all class labels being 0\n",
    "        transformed_dict[key]['No missing abnormality'] = int(all(\n",
    "            value.get(class_label, 0) == 0 for class_label in label_mapping\n",
    "        ))\n",
    "\n",
    "    return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, hamming_loss, roc_auc_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.roc_auc_score.html#roc-auc-score\n",
    "\"\"\"\n",
    "\n",
    "def generate_metrics(predictions: list[dict], ground_truth: list[dict]):\n",
    "    # Predictions and ground truth data\n",
    "\n",
    "    # Extract labels from ground truth and predictions\n",
    "    # ['Missed abnormality due to missing fixation', 'Missed abnormality due to reduced fixation', 'Missed abnormality due to incomplete knowledge', 'No missing abnormality']\n",
    "    labels = list(ground_truth[0].keys())\n",
    "\n",
    "    y_true = np.array([[gt[label] for label in labels] for gt in ground_truth])\n",
    "    y_pred = np.array([[pred[label] for label in labels] for pred in predictions])\n",
    "\n",
    "    # Calculate multilabel classification metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=labels, digits=4))\n",
    "\n",
    "    print(\"\\nAccuracy Score:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_true, y_pred))\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr'))\n",
    "    print()\n",
    "\n",
    "    # Calculate ROC AUC and Precision-Recall AUC for each label\n",
    "    for i, label in enumerate(labels):\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "            avg_precision = average_precision_score(y_true[:, i], y_pred[:, i])\n",
    "            accuracy = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "            print(f\"Accuracy for {label}: {accuracy}\")\n",
    "            print(f\"ROC AUC for {label}: {roc_auc}\")\n",
    "            print(\"Hamming loss\", hamming_loss(y_true[:, i], y_pred[:, i]))\n",
    "            print(\"--------------------------------------------------\")\n",
    "        \n",
    "        except ValueError:\n",
    "            print(f\"\\nROC AUC and Average Precision for {label} could not be calculated due to lack of positive samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_inference_time(predictions: list[dict]):\n",
    "    \"\"\"\n",
    "    Calculate the average inference time from the predictions.\n",
    "    Assumes that each prediction dictionary contains an 'inference_time' key.\n",
    "    \"\"\"\n",
    "    total_time = 0\n",
    "    count = 0\n",
    "\n",
    "    for pred in predictions:\n",
    "        if 'inference_time' in pred:\n",
    "            total_time += pred['inference_time']\n",
    "            count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return 0\n",
    "\n",
    "    return total_time / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def evaluate(results_file_path: str, ground_truth_metadata: dict):\n",
    "    with open (results_file_path) as file:\n",
    "        results = json.load(file)\n",
    "    \n",
    "    predictions = []\n",
    "    transformed_gt = transform_label_dict(ground_truth_metadata)\n",
    "    ground_truth = []\n",
    "\n",
    "    for dicom_id, pred in results.items():\n",
    "        predictions.append(pred)\n",
    "        ground_truth.append(transformed_gt[dicom_id])\n",
    "\n",
    "    assert len(predictions) == len(ground_truth)\n",
    "\n",
    "    print(\"Average Inference Time:\", round(get_average_inference_time(predictions), 3), \"seconds\")\n",
    "\n",
    "    generate_metrics(predictions, ground_truth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Replace with the actual file paths\n",
    "\n",
    "with open(\"../original_fixation_transcript_metadata.json\", 'r') as file:\n",
    "    orig_xy_ground_truth_metadata = json.load(file)\n",
    "\n",
    "with open(\"../original_fixation_transcript_data.json\", 'r') as file:\n",
    "    orig_xy_fixation_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA-3.2-11B-Vision-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o-Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph of Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Time: 4.653 seconds\n",
      "Classification Report:\n",
      "                                                precision    recall  f1-score   support\n",
      "\n",
      "    Missed abnormality due to missing fixation     0.5652    0.1806    0.2737       432\n",
      "    Missed abnormality due to reduced fixation     0.5893    0.9167    0.7174       432\n",
      "Missed abnormality due to incomplete knowledge     0.2054    0.8447    0.3305       161\n",
      "                        No missing abnormality     0.5496    1.0000    0.7094       216\n",
      "\n",
      "                                     micro avg     0.4429    0.6656    0.5319      1241\n",
      "                                     macro avg     0.4774    0.7355    0.5077      1241\n",
      "                                  weighted avg     0.5242    0.6656    0.5113      1241\n",
      "                                   samples avg     0.4980    0.6868    0.5600      1241\n",
      "\n",
      "\n",
      "Accuracy Score: 0.2175609756097561\n",
      "Hamming Loss: 0.3546341463414634\n",
      "ROC AUC Score: 0.6934684174465799\n",
      "\n",
      "Accuracy for Missed abnormality due to missing fixation: 0.5960975609756097\n",
      "ROC AUC for Missed abnormality due to missing fixation: 0.5396875585534945\n",
      "Hamming loss 0.40390243902439027\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to reduced fixation: 0.6956097560975609\n",
      "ROC AUC for Missed abnormality due to reduced fixation: 0.7256183249016299\n",
      "Hamming loss 0.304390243902439\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to incomplete knowledge: 0.4624390243902439\n",
      "ROC AUC for Missed abnormality due to incomplete knowledge: 0.6179621002990568\n",
      "Hamming loss 0.5375609756097561\n",
      "--------------------------------------------------\n",
      "Accuracy for No missing abnormality: 0.8273170731707317\n",
      "ROC AUC for No missing abnormality: 0.8906056860321384\n",
      "Hamming loss 0.17268292682926828\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"../gotcot/gpt4o_mini_got_results.json\", orig_xy_ground_truth_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
