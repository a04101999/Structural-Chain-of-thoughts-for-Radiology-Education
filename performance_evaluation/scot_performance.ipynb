{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install scikit-learn\n",
    "% pip install matplotlib\n",
    "% pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupiter notebook evaluates the performance of the SCot framework on the SCoT framework against standard CoT prompting in zero-shot and few-shot settings on the synthesized error dataset, highlighting its effectiveness\n",
    "in improving multimodal reasoning across different LLM/LMM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the path's with the correct paths to your results generated using each prompting method with the respective models. Wherever the path is mentioned `with open`, please replace with your path to the respective file.\n",
    "\n",
    "```python\n",
    "with open(\"orig_xy_fixation_transcript_metadata.json\", 'r') as file:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label_dict(label_dict):\n",
    "    # Define the mapping between class labels and the corresponding descriptions\n",
    "\n",
    "    \"\"\"\n",
    "    - \"No Missing Subgraph\": Set this to 1 if there are no missing subgraphs in the list. Otherwise, set it to 0.\n",
    "    - \"Missing Subgraph due to Missing fixation\": Set to 1 if there are any missing fixation points, otherwise set it to 0.\n",
    "    - \"Missing Subgraph due to reduced fixation duration\": Set to 1 if any fixation duration is shorter than expected, otherwise set it to 0.\n",
    "    - \"Missing Subgraph due to undefined reason\": Set to 1 only if there are missing subgraphs with no clear reason (i.e., no missing fixation points or reduced durations). Set it to 0 if there are clear reasons (such as missing fixation points or reduced durations).\n",
    "    \"\"\"\n",
    "    label_mapping = {\n",
    "        'class_label_1': 'Missing Subgraph due to Missing fixation',\n",
    "        'class_label_2': 'Missing Subgraph due to reduced fixation duration',\n",
    "        'class_label_3': 'Missing Subgraph due to undefined reason',\n",
    "    }\n",
    "    \n",
    "    transformed_dict = {}\n",
    "\n",
    "    for key, value in label_dict.items():\n",
    "        transformed_dict[key] = {}\n",
    "        \n",
    "        # Map each label to its new description based on `label_mapping`\n",
    "        for class_label, new_description in label_mapping.items():\n",
    "            transformed_dict[key][new_description] = value.get(class_label, 0)\n",
    "        \n",
    "        # Set \"No missing abnormality\" based on all class labels being 0\n",
    "        transformed_dict[key]['No Missing Subgraph'] = int(all(\n",
    "            value.get(class_label, 0) == 0 for class_label in label_mapping\n",
    "        ))\n",
    "\n",
    "    return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, hamming_loss, roc_auc_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.roc_auc_score.html#roc-auc-score\n",
    "\"\"\"\n",
    "\n",
    "def generate_metrics(predictions: list[dict], ground_truth: list[dict]):\n",
    "    # Predictions and ground truth data\n",
    "\n",
    "    # Extract labels from ground truth and predictions\n",
    "\n",
    "    \"\"\"\n",
    "    SCot Prompt 2: X_Original, Y_Original\n",
    "    {\n",
    "        \"No Missing Subgraph\": 0,\n",
    "        \"Missing Subgraph due to Missing fixation\": 1,\n",
    "        \"Missing Subgraph due to reduced fixation duration\": 1,\n",
    "        \"Missing Subgraph due to undefined reason\": 0\n",
    "    }\n",
    "    \"\"\"\n",
    "    labels = list(ground_truth[0].keys())\n",
    "\n",
    "    # Convert to array of [0, 1] labels based on key-value pairs, order doesn't matter\n",
    "    y_true = np.array([[gt[label] for label in labels] for gt in ground_truth])\n",
    "    y_pred = np.array([[pred[label] for label in labels] for pred in predictions])\n",
    "\n",
    "    # Calculate multilabel classification metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=labels, digits=4))\n",
    "\n",
    "    print(\"\\nAccuracy Score:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_true, y_pred))\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr'))\n",
    "    print()\n",
    "\n",
    "    # Calculate ROC AUC and Precision-Recall AUC for each label\n",
    "    for i, label in enumerate(labels):\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "            avg_precision = average_precision_score(y_true[:, i], y_pred[:, i])\n",
    "            accuracy = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "            print(f\"Accuracy for {label}: {accuracy}\")\n",
    "            print(f\"ROC AUC for {label}: {roc_auc}\")\n",
    "            print(\"Hamming loss\", hamming_loss(y_true[:, i], y_pred[:, i]))\n",
    "            print(\"----------------------------------------------\")\n",
    "        except ValueError:\n",
    "            print(f\"\\nROC AUC and Average Precision for {label} could not be calculated due to lack of positive samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_inference_time(predictions: list[dict]):\n",
    "    \"\"\"\n",
    "    Calculate the average inference time from the predictions.\n",
    "    Assumes that each prediction dictionary contains an 'inference_time' key.\n",
    "    \"\"\"\n",
    "    total_time = 0\n",
    "    count = 0\n",
    "\n",
    "    for pred in predictions:\n",
    "        if 'inference_time' in pred:\n",
    "            total_time += pred['inference_time']\n",
    "            count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return 0\n",
    "\n",
    "    return total_time / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def evaluate(results_file_path: str, ground_truth_metadata: dict):\n",
    "    with open (results_file_path) as file:\n",
    "        results = json.load(file)\n",
    "    \n",
    "    predictions = []\n",
    "    transformed_gt = transform_label_dict(ground_truth_metadata)\n",
    "    ground_truth = []\n",
    "\n",
    "    for dicom_id, pred in results.items():\n",
    "        predictions.append(pred)\n",
    "        ground_truth.append(transformed_gt[dicom_id])\n",
    "\n",
    "    assert len(predictions) == len(ground_truth)\n",
    "\n",
    "    print(\"Average Inference Time:\", round(get_average_inference_time(predictions), 3), \"seconds\")\n",
    "\n",
    "    generate_metrics(predictions, ground_truth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Replace with the actual file paths\n",
    "\n",
    "with open(\"orig_xy_fixation_transcript_metadata.json\", 'r') as file:\n",
    "    orig_xy_ground_truth_metadata = json.load(file)\n",
    "\n",
    "with open(\"orig_xy_fixation_transcript_data.json\", 'r') as file:\n",
    "    orig_xy_fixation_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA-3.2-11B-Vision-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCot Prompt: TG and P | X_Original Y_Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025\n",
      "Classification Report:\n",
      "                                                   precision    recall  f1-score   support\n",
      "\n",
      "                              No Missing Subgraph     0.9227    0.9954    0.9577       216\n",
      "         Missing Subgraph due to Missing fixation     0.9332    0.9699    0.9512       432\n",
      "Missing Subgraph due to reduced fixation duration     0.9451    0.7176    0.8158       432\n",
      "         Missing Subgraph due to undefined reason     0.7512    0.9379    0.8343       161\n",
      "\n",
      "                                        micro avg     0.9042    0.8824    0.8931      1241\n",
      "                                        macro avg     0.8881    0.9052    0.8897      1241\n",
      "                                     weighted avg     0.9119    0.8824    0.8900      1241\n",
      "                                      samples avg     0.9099    0.9015    0.8925      1241\n",
      "\n",
      "\n",
      "Accuracy Score: 0.8\n",
      "Hamming Loss: 0.06390243902439025\n",
      "ROC AUC Score: 0.9324617696277996\n",
      "\n",
      "Accuracy for No Missing Subgraph: 0.9814634146341463\n",
      "ROC AUC for No Missing Subgraph: 0.9865603396969281\n",
      "Hamming loss 0.018536585365853658\n",
      "----------------------------------------------\n",
      "Accuracy for Missing Subgraph due to Missing fixation: 0.9580487804878048\n",
      "ROC AUC for Missing Subgraph due to Missing fixation: 0.9596585940915621\n",
      "Hamming loss 0.041951219512195125\n",
      "----------------------------------------------\n",
      "Accuracy for Missing Subgraph due to reduced fixation duration: 0.8634146341463415\n",
      "ROC AUC for Missing Subgraph due to reduced fixation duration: 0.8436192305290112\n",
      "Hamming loss 0.13658536585365855\n",
      "----------------------------------------------\n",
      "Accuracy for Missing Subgraph due to undefined reason: 0.9414634146341463\n",
      "ROC AUC for Missing Subgraph due to undefined reason: 0.9400089141936969\n",
      "Hamming loss 0.05853658536585366\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brandon Chung\\Desktop\\HULA_egd_cxr\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate(\".\\llama3.211BV\\llama3.2_11B_scot_results.json\", orig_xy_ground_truth_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCot Prompt: TG and P | X_Original Y_Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025\n",
      "Classification Report:\n",
      "                                                   precision    recall  f1-score   support\n",
      "\n",
      "                              No Missing Subgraph     0.8276    1.0000    0.9057       216\n",
      "         Missing Subgraph due to Missing fixation     0.9053    0.9514    0.9278       432\n",
      "Missing Subgraph due to reduced fixation duration     0.8323    0.8958    0.8629       432\n",
      "         Missing Subgraph due to undefined reason     0.8350    0.5342    0.6515       161\n",
      "\n",
      "                                        micro avg     0.8574    0.8864    0.8716      1241\n",
      "                                        macro avg     0.8500    0.8453    0.8370      1241\n",
      "                                     weighted avg     0.8572    0.8864    0.8655      1241\n",
      "                                      samples avg     0.8564    0.8854    0.8595      1241\n",
      "\n",
      "\n",
      "Accuracy Score: 0.7639024390243903\n",
      "Hamming Loss: 0.07902439024390244\n",
      "ROC AUC Score: 0.8877545427500372\n",
      "\n",
      "Accuracy for No Missing Subgraph: 0.9560975609756097\n",
      "ROC AUC for No Missing Subgraph: 0.9721878862793571\n",
      "Hamming loss 0.04390243902439024\n",
      "----------------------------------------------\n",
      "Accuracy for Missing Subgraph due to Missing fixation: 0.937560975609756\n",
      "ROC AUC for Missing Subgraph due to Missing fixation: 0.9394381206670414\n",
      "Hamming loss 0.0624390243902439\n",
      "----------------------------------------------\n",
      "Accuracy for Missing Subgraph due to reduced fixation duration: 0.88\n",
      "ROC AUC for Missing Subgraph due to reduced fixation duration: 0.8821493816750984\n",
      "Hamming loss 0.12\n",
      "----------------------------------------------\n",
      "Accuracy for Missing Subgraph due to undefined reason: 0.9102439024390244\n",
      "ROC AUC for Missing Subgraph due to undefined reason: 0.7572427823786518\n",
      "Hamming loss 0.0897560975609756\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brandon Chung\\Desktop\\HULA_egd_cxr\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate(\".\\mistral\\mistral_7B_scot_results.json\", orig_xy_ground_truth_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o-Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCot Prompt: TG and P | X_Original Y_Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025\n",
      "Classification Report:\n",
      "                                                   precision    recall  f1-score   support\n",
      "\n",
      "                              No Missing Subgraph     0.9600    1.0000    0.9796       216\n",
      "         Missing Subgraph due to Missing fixation     0.9953    0.9792    0.9872       432\n",
      "Missing Subgraph due to reduced fixation duration     1.0000    0.9653    0.9823       432\n",
      "         Missing Subgraph due to undefined reason     0.9494    0.9317    0.9404       161\n",
      "\n",
      "                                        micro avg     0.9845    0.9718    0.9781      1241\n",
      "                                        macro avg     0.9762    0.9690    0.9724      1241\n",
      "                                     weighted avg     0.9848    0.9718    0.9781      1241\n",
      "                                      samples avg     0.9717    0.9698    0.9701      1241\n",
      "\n",
      "\n",
      "Accuracy Score: 0.9648780487804878\n",
      "Hamming Loss: 0.013170731707317073\n",
      "ROC AUC Score: 0.9815455846311062\n",
      "\n",
      "Accuracy for No Missing Subgraph: 0.9912195121951219\n",
      "ROC AUC for No Missing Subgraph: 0.9944375772558715\n",
      "Hamming loss 0.00878048780487805\n",
      "----------------------------------------------\n",
      "Accuracy for Missing Subgraph due to Missing fixation: 0.9892682926829268\n",
      "ROC AUC for Missing Subgraph due to Missing fixation: 0.9878969926925237\n",
      "Hamming loss 0.010731707317073172\n",
      "----------------------------------------------\n",
      "Accuracy for Missing Subgraph due to reduced fixation duration: 0.9853658536585366\n",
      "ROC AUC for Missing Subgraph due to reduced fixation duration: 0.9826388888888888\n",
      "Hamming loss 0.014634146341463415\n",
      "----------------------------------------------\n",
      "Accuracy for Missing Subgraph due to undefined reason: 0.9814634146341463\n",
      "ROC AUC for Missing Subgraph due to undefined reason: 0.9612088796871404\n",
      "Hamming loss 0.018536585365853658\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brandon Chung\\Desktop\\HULA_egd_cxr\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate(\".\\gpt4o_mini\\gpt4o_mini_scot_results.json\", orig_xy_ground_truth_metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
