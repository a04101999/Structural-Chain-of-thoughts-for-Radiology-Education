{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install scikit-learn\n",
    "% pip install matplotlib\n",
    "% pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupiter notebook evaluates the performance of the baseline zero-shot and few-shot Tree of Thoughts prompting on the synthesized error dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the path's with the correct paths to your results generated using each prompting method with the respective models. Wherever the path is mentioned `with open`, please replace with your path to the respective file.\n",
    "\n",
    "```python\n",
    "with open(\".\\llama3.211BV\\llama3.2_11B_zshot_predictions.json\") as file:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label_dict(label_dict):\n",
    "    # Define the mapping between class labels and the corresponding descriptions\n",
    "    label_mapping = {\n",
    "        'class_label_1': 'Missed abnormality due to missing fixation',\n",
    "        'class_label_2': 'Missed abnormality due to reduced fixation',\n",
    "        'class_label_3': 'Missed abnormality due to incomplete knowledge'\n",
    "    }\n",
    "\n",
    "    transformed_dict = {}\n",
    "\n",
    "    for key, value in label_dict.items():\n",
    "        transformed_dict[key] = {}\n",
    "\n",
    "        # Map each label to its new description based on `label_mapping`\n",
    "        for class_label, new_description in label_mapping.items():\n",
    "            transformed_dict[key][new_description] = value.get(class_label, 0)\n",
    "        \n",
    "        # Set \"No missing abnormality\" based on all class labels being 0\n",
    "        transformed_dict[key]['No missing abnormality'] = int(all(\n",
    "            value.get(class_label, 0) == 0 for class_label in label_mapping\n",
    "        ))\n",
    "\n",
    "    return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, hamming_loss, roc_auc_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.roc_auc_score.html#roc-auc-score\n",
    "\"\"\"\n",
    "\n",
    "def generate_metrics(predictions: list[dict], ground_truth: list[dict]):\n",
    "    # Predictions and ground truth data\n",
    "\n",
    "    # Extract labels from ground truth and predictions\n",
    "    # ['Missed abnormality due to missing fixation', 'Missed abnormality due to reduced fixation', 'Missed abnormality due to incomplete knowledge', 'No missing abnormality']\n",
    "    labels = list(ground_truth[0].keys())\n",
    "\n",
    "    y_true = np.array([[gt[label] for label in labels] for gt in ground_truth])\n",
    "    y_pred = np.array([[pred[label] for label in labels] for pred in predictions])\n",
    "\n",
    "    # Calculate multilabel classification metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=labels, digits=4))\n",
    "\n",
    "    print(\"\\nAccuracy Score:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_true, y_pred))\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr'))\n",
    "    print()\n",
    "\n",
    "    # Calculate ROC AUC and Precision-Recall AUC for each label\n",
    "    for i, label in enumerate(labels):\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "            avg_precision = average_precision_score(y_true[:, i], y_pred[:, i])\n",
    "            accuracy = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "            print(f\"Accuracy for {label}: {accuracy}\")\n",
    "            print(f\"ROC AUC for {label}: {roc_auc}\")\n",
    "            print(\"Hamming loss\", hamming_loss(y_true[:, i], y_pred[:, i]))\n",
    "            print(\"--------------------------------------------------\")\n",
    "        \n",
    "        except ValueError:\n",
    "            print(f\"\\nROC AUC and Average Precision for {label} could not be calculated due to lack of positive samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_inference_time(predictions: list[dict]):\n",
    "    \"\"\"\n",
    "    Calculate the average inference time from the predictions.\n",
    "    Assumes that each prediction dictionary contains an 'inference_time' key.\n",
    "    \"\"\"\n",
    "    total_time = 0\n",
    "    count = 0\n",
    "\n",
    "    for pred in predictions:\n",
    "        if 'inference_time' in pred:\n",
    "            total_time += pred['inference_time']\n",
    "            count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return 0\n",
    "\n",
    "    return total_time / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def evaluate(results_file_path: str, ground_truth_metadata: dict):\n",
    "    with open (results_file_path) as file:\n",
    "        results = json.load(file)\n",
    "    \n",
    "    predictions = []\n",
    "    transformed_gt = transform_label_dict(ground_truth_metadata)\n",
    "    ground_truth = []\n",
    "\n",
    "    for dicom_id, pred in results.items():\n",
    "        predictions.append(pred)\n",
    "        ground_truth.append(transformed_gt[dicom_id])\n",
    "\n",
    "    assert len(predictions) == len(ground_truth)\n",
    "\n",
    "    print(\"Average Inference Time:\", round(get_average_inference_time(predictions), 3), \"seconds\")\n",
    "\n",
    "    generate_metrics(predictions, ground_truth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Replace with the actual file paths\n",
    "\n",
    "with open(\"../original_fixation_transcript_metadata.json\", 'r') as file:\n",
    "    orig_xy_ground_truth_metadata = json.load(file)\n",
    "\n",
    "with open(\"../original_fixation_transcript_data.json\", 'r') as file:\n",
    "    orig_xy_fixation_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA-3.2-11B-Vision-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o-Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree of Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Time: 5.938 seconds\n",
      "Classification Report:\n",
      "                                                precision    recall  f1-score   support\n",
      "\n",
      "    Missed abnormality due to missing fixation     0.5792    0.3218    0.4137       432\n",
      "    Missed abnormality due to reduced fixation     0.4435    0.9907    0.6127       432\n",
      "Missed abnormality due to incomplete knowledge     0.1571    1.0000    0.2715       161\n",
      "                        No missing abnormality     1.0000    0.0417    0.0800       216\n",
      "\n",
      "                                     micro avg     0.3292    0.5939    0.4236      1241\n",
      "                                     macro avg     0.5449    0.5885    0.3445      1241\n",
      "                                  weighted avg     0.5504    0.5939    0.4065      1241\n",
      "                                   samples avg     0.3115    0.5746    0.3937      1241\n",
      "\n",
      "\n",
      "Accuracy Score: 0.0\n",
      "Hamming Loss: 0.4892682926829268\n",
      "ROC AUC Score: 0.5347851672287802\n",
      "\n",
      "Accuracy for Missed abnormality due to missing fixation: 0.615609756097561\n",
      "ROC AUC for Missed abnormality due to missing fixation: 0.5757194272687527\n",
      "Hamming loss 0.38439024390243903\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to reduced fixation: 0.4721951219512195\n",
      "ROC AUC for Missed abnormality due to reduced fixation: 0.5425879083130348\n",
      "Hamming loss 0.5278048780487805\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to incomplete knowledge: 0.15707317073170732\n",
      "ROC AUC for Missed abnormality due to incomplete knowledge: 0.5\n",
      "Hamming loss 0.8429268292682927\n",
      "--------------------------------------------------\n",
      "Accuracy for No missing abnormality: 0.7980487804878049\n",
      "ROC AUC for No missing abnormality: 0.5208333333333334\n",
      "Hamming loss 0.20195121951219513\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"../totcot/gpt4o_mini_tot_zero_shot_results.json\", orig_xy_ground_truth_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Time: 7.457 seconds\n",
      "Classification Report:\n",
      "                                                precision    recall  f1-score   support\n",
      "\n",
      "    Missed abnormality due to missing fixation     0.6650    0.3032    0.4165       432\n",
      "    Missed abnormality due to reduced fixation     0.7326    0.3171    0.4426       432\n",
      "Missed abnormality due to incomplete knowledge     0.1789    1.0000    0.3035       161\n",
      "                        No missing abnormality     1.0000    0.5602    0.7181       216\n",
      "\n",
      "                                     micro avg     0.3915    0.4432    0.4157      1241\n",
      "                                     macro avg     0.6441    0.5451    0.4702      1241\n",
      "                                  weighted avg     0.6838    0.4432    0.4634      1241\n",
      "                                   samples avg     0.3859    0.4576    0.4098      1241\n",
      "\n",
      "\n",
      "Accuracy Score: 0.2370731707317073\n",
      "Hamming Loss: 0.3770731707317073\n",
      "ROC AUC Score: 0.6412019958934483\n",
      "\n",
      "Accuracy for Missed abnormality due to missing fixation: 0.6419512195121951\n",
      "ROC AUC for Missed abnormality due to missing fixation: 0.5959711292236587\n",
      "Hamming loss 0.35804878048780486\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to reduced fixation: 0.6634146341463415\n",
      "ROC AUC for Missed abnormality due to reduced fixation: 0.6164062987945788\n",
      "Hamming loss 0.33658536585365856\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to incomplete knowledge: 0.27902439024390246\n",
      "ROC AUC for Missed abnormality due to incomplete knowledge: 0.572337962962963\n",
      "Hamming loss 0.7209756097560975\n",
      "--------------------------------------------------\n",
      "Accuracy for No missing abnormality: 0.9073170731707317\n",
      "ROC AUC for No missing abnormality: 0.7800925925925926\n",
      "Hamming loss 0.09268292682926829\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"../totcot/gpt4o_mini_tot_few_shot_results.json\", orig_xy_ground_truth_metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
