{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install scikit-learn\n",
    "% pip install matplotlib\n",
    "% pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupiter notebook evaluates the performance of the baseline zero-shot and few-shot prompting on the synthesized error dataset, to compare the effectiveness of the SCot framework in improving multimodal reasoning across different LLM/LMM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the path's with the correct paths to your results generated using each prompting method with the respective models. Wherever the path is mentioned `with open`, please replace with your path to the respective file.\n",
    "\n",
    "```python\n",
    "with open(\".\\llama3.211BV\\llama3.2_11B_zshot_predictions.json\") as file:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label_dict(label_dict):\n",
    "    # Define the mapping between class labels and the corresponding descriptions\n",
    "    label_mapping = {\n",
    "        'class_label_1': 'Missed abnormality due to missing fixation',\n",
    "        'class_label_2': 'Missed abnormality due to reduced fixation',\n",
    "        'class_label_3': 'Missed abnormality due to incomplete knowledge'\n",
    "    }\n",
    "\n",
    "    transformed_dict = {}\n",
    "\n",
    "    for key, value in label_dict.items():\n",
    "        transformed_dict[key] = {}\n",
    "\n",
    "        # Map each label to its new description based on `label_mapping`\n",
    "        for class_label, new_description in label_mapping.items():\n",
    "            transformed_dict[key][new_description] = value.get(class_label, 0)\n",
    "        \n",
    "        # Set \"No missing abnormality\" based on all class labels being 0\n",
    "        transformed_dict[key]['No missing abnormality'] = int(all(\n",
    "            value.get(class_label, 0) == 0 for class_label in label_mapping\n",
    "        ))\n",
    "\n",
    "    return transformed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, hamming_loss, roc_auc_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.roc_auc_score.html#roc-auc-score\n",
    "\"\"\"\n",
    "\n",
    "def generate_metrics(predictions: list[dict], ground_truth: list[dict]):\n",
    "    # Predictions and ground truth data\n",
    "\n",
    "    # Extract labels from ground truth and predictions\n",
    "    # ['Missed abnormality due to missing fixation', 'Missed abnormality due to reduced fixation', 'Missed abnormality due to incomplete knowledge', 'No missing abnormality']\n",
    "    labels = list(ground_truth[0].keys())\n",
    "\n",
    "    y_true = np.array([[gt[label] for label in labels] for gt in ground_truth])\n",
    "    y_pred = np.array([[pred[label] for label in labels] for pred in predictions])\n",
    "\n",
    "    # Calculate multilabel classification metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=labels, digits=4))\n",
    "\n",
    "    print(\"\\nAccuracy Score:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_true, y_pred))\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr'))\n",
    "    print()\n",
    "\n",
    "    # Calculate ROC AUC and Precision-Recall AUC for each label\n",
    "    for i, label in enumerate(labels):\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "            avg_precision = average_precision_score(y_true[:, i], y_pred[:, i])\n",
    "            accuracy = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "            print(f\"Accuracy for {label}: {accuracy}\")\n",
    "            print(f\"ROC AUC for {label}: {roc_auc}\")\n",
    "            print(\"Hamming loss\", hamming_loss(y_true[:, i], y_pred[:, i]))\n",
    "            print(\"--------------------------------------------------\")\n",
    "        \n",
    "        except ValueError:\n",
    "            print(f\"\\nROC AUC and Average Precision for {label} could not be calculated due to lack of positive samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_inference_time(predictions: list[dict]):\n",
    "    \"\"\"\n",
    "    Calculate the average inference time from the predictions.\n",
    "    Assumes that each prediction dictionary contains an 'inference_time' key.\n",
    "    \"\"\"\n",
    "    total_time = 0\n",
    "    count = 0\n",
    "\n",
    "    for pred in predictions:\n",
    "        if 'inference_time' in pred:\n",
    "            total_time += pred['inference_time']\n",
    "            count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return 0\n",
    "\n",
    "    return total_time / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def evaluate(results_file_path: str, ground_truth_metadata: dict):\n",
    "    with open (results_file_path) as file:\n",
    "        results = json.load(file)\n",
    "    \n",
    "    predictions = []\n",
    "    transformed_gt = transform_label_dict(ground_truth_metadata)\n",
    "    ground_truth = []\n",
    "\n",
    "    for dicom_id, pred in results.items():\n",
    "        predictions.append(pred)\n",
    "        ground_truth.append(transformed_gt[dicom_id])\n",
    "\n",
    "    assert len(predictions) == len(ground_truth)\n",
    "\n",
    "    print(\"Average Inference Time:\", round(get_average_inference_time(predictions), 3), \"seconds\")\n",
    "\n",
    "    generate_metrics(predictions, ground_truth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Replace with the actual file paths\n",
    "\n",
    "with open(\"../original_fixation_transcript_metadata.json\", 'r') as file:\n",
    "    orig_xy_ground_truth_metadata = json.load(file)\n",
    "\n",
    "with open(\"../original_fixation_transcript_data.json\", 'r') as file:\n",
    "    orig_xy_fixation_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA-3.2-11B-Vision-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o-Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree of Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Time: 5.242 seconds\n",
      "Classification Report:\n",
      "                                                precision    recall  f1-score   support\n",
      "\n",
      "    Missed abnormality due to missing fixation     0.0000    0.0000    0.0000       432\n",
      "    Missed abnormality due to reduced fixation     0.4386    1.0000    0.6097       432\n",
      "Missed abnormality due to incomplete knowledge     0.1599    1.0000    0.2757       161\n",
      "                        No missing abnormality     1.0000    0.1759    0.2992       216\n",
      "\n",
      "                                     micro avg     0.3107    0.5085    0.3857      1241\n",
      "                                     macro avg     0.3996    0.5440    0.2962      1241\n",
      "                                  weighted avg     0.3475    0.5085    0.3001      1241\n",
      "                                   samples avg     0.3078    0.5102    0.3753      1241\n",
      "\n",
      "\n",
      "Accuracy Score: 0.0\n",
      "Hamming Loss: 0.4902439024390244\n",
      "ROC AUC Score: 0.5328158180313535\n",
      "\n",
      "Accuracy for Missed abnormality due to missing fixation: 0.577560975609756\n",
      "ROC AUC for Missed abnormality due to missing fixation: 0.4991568296795953\n",
      "Hamming loss 0.4224390243902439\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to reduced fixation: 0.4604878048780488\n",
      "ROC AUC for Missed abnormality due to reduced fixation: 0.533726812816189\n",
      "Hamming loss 0.5395121951219513\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to incomplete knowledge: 0.1746341463414634\n",
      "ROC AUC for Missed abnormality due to incomplete knowledge: 0.5104166666666667\n",
      "Hamming loss 0.8253658536585365\n",
      "--------------------------------------------------\n",
      "Accuracy for No missing abnormality: 0.8263414634146341\n",
      "ROC AUC for No missing abnormality: 0.587962962962963\n",
      "Hamming loss 0.17365853658536584\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "evaluate(\"../totcot/gpt4o_mini_tot_results.json\", orig_xy_ground_truth_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                                                precision    recall  f1-score   support\n",
      "\n",
      "    Missed abnormality due to missing fixation     0.5519    0.9477    0.6976       421\n",
      "    Missed abnormality due to reduced fixation     0.6901    0.7458    0.7169       421\n",
      "Missed abnormality due to incomplete knowledge     0.1860    0.1491    0.1655       161\n",
      "                        No missing abnormality     0.8120    1.0000    0.8963       216\n",
      "\n",
      "                                     micro avg     0.6058    0.7818    0.6827      1219\n",
      "                                     macro avg     0.5600    0.7107    0.6191      1219\n",
      "                                  weighted avg     0.5974    0.7818    0.6692      1219\n",
      "                                   samples avg     0.6157    0.7406    0.6555      1219\n",
      "\n",
      "\n",
      "Accuracy Score: 0.4832347140039448\n",
      "Hamming Loss: 0.21844181459566075\n",
      "ROC AUC Score: 0.7340943778462388\n",
      "\n",
      "Accuracy for Missed abnormality due to missing fixation: 0.6587771203155819\n",
      "ROC AUC for Missed abnormality due to missing fixation: 0.700684550155616\n",
      "Hamming loss 0.34122287968441817\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to reduced fixation: 0.7554240631163708\n",
      "ROC AUC for Missed abnormality due to reduced fixation: 0.7540346000248344\n",
      "Hamming loss 0.2445759368836292\n",
      "--------------------------------------------------\n",
      "Accuracy for Missed abnormality due to incomplete knowledge: 0.7613412228796844\n",
      "ROC AUC for Missed abnormality due to incomplete knowledge: 0.5129866820065098\n",
      "Hamming loss 0.23865877712031558\n",
      "--------------------------------------------------\n",
      "Accuracy for No missing abnormality: 0.9506903353057199\n",
      "ROC AUC for No missing abnormality: 0.968671679197995\n",
      "Hamming loss 0.04930966469428008\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Replace with the actual file paths\n",
    "\n",
    "with open (\".\\gpt4o\\gpt4o_mini_fshot_predictions.json\") as file:\n",
    "    gpt4oMini_saved_fshot_results = json.load(file)\n",
    "\n",
    "predictions = []\n",
    "transformed_gt = transform_label_dict(orig_xy_ground_truth_metadata)\n",
    "ground_truth = []\n",
    "\n",
    "for dicom_id, pred in gpt4oMini_saved_fshot_results.items():\n",
    "    predictions.append(pred)\n",
    "    ground_truth.append(transformed_gt[dicom_id])\n",
    "\n",
    "evaluation_metrics(predictions, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
